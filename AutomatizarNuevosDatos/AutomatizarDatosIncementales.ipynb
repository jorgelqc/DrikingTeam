{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librería que vamos a usar para la automatización del monitoreo de los datos, es Watchdog. Estuvimos averiguando y es una muy buena herramienta para identificar cambios en el sistema de archivos en tiempo real. Es particularmente útil cuando necesitas realizar acciones automáticas en respuesta a eventos como la creación, modificación, o eliminación de archivos y directorios. Es por eso, que nos decidimos por Watchdog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para interactual con la base de datos, vamos a usar pyodbc. Ver con cual resulta mas fácil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conectarse a la base de datos en cuestion\n",
    "\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Configuración de la cadena de conexión\n",
    "server = 'your_server_name.database.windows.net'\n",
    "database = 'your_db_name'\n",
    "username = 'your_username'\n",
    "password = 'your_password'\n",
    "driver = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "# Conexión con pyodbc\n",
    "conn = pyodbc.connect(f'DRIVER={driver};SERVER={server};DATABASE={database};UID={username};PWD={password}')\n",
    "\n",
    "# O, si prefieres usar SQLAlchemy para manejar la conexión y ejecutar el DataFrame\n",
    "engine = create_engine(f'mssql+pyodbc://{username}:{password}@{server}/{database}?driver=ODBC+Driver+17+for+SQL+Server')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el último registro procesado\n",
    "\n",
    "last_ingestion = pd.read_sql('SELECT TOP 1 * FROM ingestion_control ORDER BY last_ingestion_id DESC', conn)\n",
    "last_id = last_ingestion['last_ingestion_id'].iloc[0] if not last_ingestion.empty else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el CSV en un DataFrame de pandas\n",
    "df = pd.read_csv('path/to/your/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filtrar los nuevos registros que no han sido procesados\n",
    "new_data = df[df['id_column'] > last_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insertar los nuevos datos en la base de datos\n",
    "if not new_data.empty:\n",
    "    new_data.to_sql('your_table_name', engine, if_exists='append', index=False)\n",
    "\n",
    "    # Actualizar la tabla de control con el último ID procesado\n",
    "    last_processed_id = new_data['id_column'].max()\n",
    "    engine.execute(f\"INSERT INTO ingestion_control (last_ingestion_id, last_ingestion_timestamp) VALUES ({last_processed_id}, CURRENT_TIMESTAMP)\")\n",
    "else:\n",
    "    print(\"No hay nuevos datos para insertar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternativa para la ingesta de datos con GOOGLE CLOUD SERVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pyodbc\n",
    "from google.cloud import storage\n",
    "\n",
    "def process_csv(event, context):\n",
    "    # Información del archivo en GCS\n",
    "    bucket_name = event['bucket']\n",
    "    file_name = event['name']\n",
    "\n",
    "    # Conectar a Google Cloud Storage\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(file_name)\n",
    "\n",
    "    # Descargar el archivo CSV a un archivo temporal\n",
    "    local_file = f\"/tmp/{file_name}\"\n",
    "    blob.download_to_filename(local_file)\n",
    "\n",
    "    # Conectar a SQL Server\n",
    "    server = 'tcp:mi-servidor-sql-server.database.windows.net'  # Cambiar por la dirección de tu servidor SQL Server\n",
    "    database = 'mi_base_de_datos'  # Cambiar por el nombre de tu base de datos en SQL Server\n",
    "    username = 'mi_usuario'  # Cambiar por el nombre de usuario de tu base de datos SQL Server\n",
    "    password = 'mi_contraseña'  # Cambiar por la contraseña del usuario de tu base de datos SQL Server\n",
    "    driver = '{ODBC Driver 17 for SQL Server}'  # Verificar que el driver ODBC esté disponible en el entorno\n",
    "\n",
    "    conn = pyodbc.connect(\n",
    "        'DRIVER=' + driver + ';SERVER=' + server + \n",
    "        ';PORT=1433;DATABASE=' + database + \n",
    "        ';UID=' + username + \n",
    "        ';PWD=' + password\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Lógica de procesamiento dependiendo del archivo   \n",
    "    if \"ventas_limpio\" in file_name:\n",
    "        process_csv_with_executemany(local_file, cursor, 'ventas_tabla')  # Cambiar 'ventas_tabla' por el nombre de la tabla de ventas en SQL Server\n",
    "    elif \"compras_limpio\" in file_name:\n",
    "        process_csv_with_executemany(local_file, cursor, 'compras_tabla')  # Cambiar 'compras_tabla' por el nombre de la tabla de compras en SQL Server\n",
    "    elif \"inventario_inicial_limpio\" in file_name:\n",
    "        process_csv_with_executemany(local_file, cursor, 'inventario_inicial_tabla')  # Cambiar 'inventario_inicial_tabla' por el nombre de la tabla de inventario inicial en SQL Server\n",
    "    elif \"inventario_final_limpio\" in file_name:\n",
    "        process_csv_with_executemany(local_file, cursor, 'inventario_final_tabla')  # Cambiar 'inventario_final_tabla' por el nombre de la tabla de inventario final en SQL Server\n",
    "\n",
    "    # Confirmar y cerrar la conexión\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    # Eliminar archivo temporal\n",
    "    os.remove(local_file)\n",
    "\n",
    "def process_csv_with_executemany(local_file, cursor, table_name):\n",
    "    \"\"\"\n",
    "    Procesa un archivo CSV e inserta los datos en la tabla SQL Server utilizando executemany.\n",
    "    :param local_file: Ruta del archivo CSV local.\n",
    "    :param cursor: Cursor de la base de datos SQL Server.\n",
    "    :param table_name: Nombre de la tabla en SQL Server.\n",
    "    \"\"\"\n",
    "    with open(local_file, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        headers = next(reader)  # Leer los encabezados\n",
    "        data = [tuple(row) for row in reader]  # Capturar todas las columnas como tuplas\n",
    "\n",
    "    # Construir la consulta SQL\n",
    "    placeholders = ', '.join(['?' for _ in headers])\n",
    "    query = f\"INSERT INTO {table_name} VALUES ({placeholders})\"\n",
    "    \n",
    "    # Ejecutar la inserción masiva\n",
    "    cursor.executemany(query, data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Esto es para poner en la terminal cuando tenga acceso a CGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gcloud functions deploy process_csv \\\n",
    "    --runtime python310 \\\n",
    "    --trigger-resource tu-bucket \\\n",
    "    --trigger-event google.storage.object.finalize \\\n",
    "    --timeout=540s \\\n",
    "    --entry-point process_csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--runtime python310: Especifica la versión de Python.\n",
    "\n",
    "--trigger-resource: El nombre del bucket en GCS donde se suben los archivos.\n",
    "\n",
    "--trigger-event: Especifica que la función se dispare cuando un archivo se sube completamente.\n",
    "\n",
    "--timeout=540s: Ajusta el tiempo máximo que la función puede ejecutarse.\n",
    "\n",
    "--entry-point process_csv: Define la función principal que será ejecutada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué Hacer Después?\n",
    "\n",
    "Obtener Credenciales: Cuando tengas acceso a las credenciales de SQL Server y GCS, podrás completar las partes del código que dependen de estas.\n",
    "\n",
    "Pruebas Locales: Puedes configurar un entorno local que imite el comportamiento de la función en la nube para hacer pruebas preliminares.\n",
    "\n",
    "Despliegue y Monitoreo: Cuando todo esté listo, despliega la función en Google Cloud y monitorea su funcionamiento usando Cloud Logging y Monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En caso de querer probar primero GCS y despues integrar lo de SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este codigo es una opcion donde no esta la parte de SQL integrada, asi probamos la funcionalidad de GCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero la parte CGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from google.cloud import storage\n",
    "\n",
    "def process_csv(event, context):\n",
    "    # Información del archivo en GCS\n",
    "    bucket_name = event['bucket']\n",
    "    file_name = event['name']\n",
    "\n",
    "    # Conectar a Google Cloud Storage\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(file_name)\n",
    "\n",
    "    # Descargar el archivo CSV a un archivo temporal\n",
    "    local_file = f\"/tmp/{file_name}\"\n",
    "    blob.download_to_filename(local_file)\n",
    "\n",
    "    # Lógica de procesamiento dependiendo del archivo\n",
    "    if \"ventas_limpio\" in file_name:\n",
    "        data = read_csv_file(local_file)\n",
    "        # Aquí llamarías a la función SQL cuando esté integrada, por ahora solo imprime\n",
    "        print(\"Datos procesados para ventas:\", data)\n",
    "    elif \"compras_limpio\" in file_name:\n",
    "        data = read_csv_file(local_file)\n",
    "        print(\"Datos procesados para compras:\", data)\n",
    "    elif \"inventario_inicial_limpio\" in file_name:\n",
    "        data = read_csv_file(local_file)\n",
    "        print(\"Datos procesados para inventario inicial:\", data)\n",
    "    elif \"inventario_final_limpio\" in file_name:\n",
    "        data = read_csv_file(local_file)\n",
    "        print(\"Datos procesados para inventario final:\", data)\n",
    "\n",
    "    # Eliminar archivo temporal\n",
    "    os.remove(local_file)\n",
    "\n",
    "def read_csv_file(local_file):\n",
    "    \"\"\"\n",
    "    Lee el archivo CSV y retorna los datos como una lista de tuplas.\n",
    "    :param local_file: Ruta del archivo CSV local.\n",
    "    :return: Lista de tuplas con los datos.\n",
    "    \"\"\"\n",
    "    with open(local_file, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        headers = next(reader)  # Leer los encabezados\n",
    "        data = [tuple(row) for row in reader]  # Capturar todas las columnas como tuplas\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integracion del SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "\n",
    "def insert_data_into_sql(data, table_name):\n",
    "    \"\"\"\n",
    "    Inserta datos en la tabla SQL Server especificada.\n",
    "    :param data: Lista de tuplas con los datos a insertar.\n",
    "    :param table_name: Nombre de la tabla en SQL Server.\n",
    "    \"\"\"\n",
    "    # Conectar a SQL Server\n",
    "    server = 'tcp:mi-servidor-sql-server.database.windows.net'  # Cambiar por la dirección de tu servidor SQL Server\n",
    "    database = 'mi_base_de_datos'  # Cambiar por el nombre de tu base de datos en SQL Server\n",
    "    username = 'mi_usuario'  # Cambiar por el nombre de usuario de tu base de datos SQL Server\n",
    "    password = 'mi_contraseña'  # Cambiar por la contraseña del usuario de tu base de datos SQL Server\n",
    "    driver = '{ODBC Driver 17 for SQL Server}'  # Verificar que el driver ODBC esté disponible en el entorno\n",
    "\n",
    "    conn = pyodbc.connect(\n",
    "        'DRIVER=' + driver + ';SERVER=' + server + \n",
    "        ';PORT=1433;DATABASE=' + database + \n",
    "        ';UID=' + username + \n",
    "        ';PWD=' + password\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Construir la consulta SQL\n",
    "    placeholders = ', '.join(['?' for _ in data[0]])\n",
    "    query = f\"INSERT INTO {table_name} VALUES ({placeholders})\"\n",
    "    \n",
    "    # Ejecutar la inserción masiva\n",
    "    cursor.executemany(query, data)\n",
    "\n",
    "    # Confirmar y cerrar la conexión\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
